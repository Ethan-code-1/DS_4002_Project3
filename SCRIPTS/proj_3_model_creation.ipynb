{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from tqdm import tqdm  \n",
    "import time            \n",
    "import torchvision.transforms.functional as TF \n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_19th = pd.read_csv('../DATA/pt_19th.csv')\n",
    "dr_19th = pd.read_csv('../DATA/dr_19th.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function Purpose: Given a URL and desired future image size will:\n",
    "1. Fetch the corresponding image at URL\n",
    "2. Upon fetching the image will standardize it preserving aspect ratio then return the image\n",
    "\n",
    "Code developed with help of Chat GPT. We used 244 x 244 as an initial size due to online literature that suggests pre-trained model work best for images with these resolutions\n",
    "'''\n",
    "def fetch_and_standardize_image(url, size=(224, 224)):\n",
    "\n",
    "    #1. First attempt to ping the url and get the associated image\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        #2. Once you have the response use it to grab the image and for consistency make sure it is RGB every time, for testing I have added a print statement here to see original image\n",
    "        img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "       #plt.imshow(img)\n",
    "        #plt.axis('off')  # Turn off axis labels\n",
    "        #plt.title(\"Image before Resizing\")\n",
    "        #plt.show()\n",
    "\n",
    "\n",
    "        #3. Get the original dimensions of the artwork to guide resizing step\n",
    "        orig_width, orig_height = img.size\n",
    "\n",
    "        #4. This part of the code looks at the original dimensions and the desired dimensions to work to resize the image while maintaining aspect ratio\n",
    "        if orig_width > orig_height:\n",
    "            new_width = size[0]\n",
    "            new_height = int(size[1] * orig_height / orig_width)\n",
    "        else:\n",
    "            new_height = size[1]\n",
    "            new_width = int(size[0] * orig_width / orig_height)\n",
    "\n",
    "        #5. Resize the image now to the newly calculated dimensions \n",
    "        img_resized = img.resize((new_width, new_height), Image.BICUBIC)\n",
    "\n",
    "        #6. Pad the image to match the desired size (adds padding where necissary to ensure the image fits within the target size specified in function parameters)\n",
    "        img_padded = ImageOps.pad(img_resized, size, color=(255, 255, 255))\n",
    "\n",
    "        #7. Showcases the image that has been resized, this is before the next step which will then convert the image into a numeric format for use in nueral net\n",
    "        #plt.imshow(img_padded)\n",
    "        #plt.axis('off')  # Turn off axis labels\n",
    "        #plt.title(\"Image After Resizing\")\n",
    "        #plt.show()\n",
    "\n",
    "        ''''\n",
    "        #7. Prepares the image data to be used in a nueral network, changing an image into numbers so the returned image can be used in the machine learning model\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(), \n",
    "        ])\n",
    "        '''\n",
    "\n",
    "        #return transform(img_padded)\n",
    "        img_array = img_to_array(img_padded)\n",
    "        return img_array\n",
    "\n",
    "    #Additional error handling as needed \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching or processing image: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test invocation of the fetch and standardize image function \n",
    "print(pt_19th.iloc[1].URL)\n",
    "test_url = pt_19th.iloc[1].URL\n",
    "test_image = fetch_and_standardize_image(test_url)\n",
    "print(\"Function returns the tensor image : \", test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Added a binary encoding where paintings are 0 and drawings are 1 in a label column so when merged have a easy variable to reference for model\n",
    "pt_19th['label'] = 0  # Paintings\n",
    "dr_19th['label'] = 1  # Drawings\n",
    "\n",
    "#2. Combine the painting and drawing dataframes so we have one dataframe. Sample function used to randomize order of joined dataframe\n",
    "all_data = pd.concat([pt_19th, dr_19th]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "#3. Testing to see new resulting dataframe, because randomization with sample can see both paintings and drawings at top\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Create an empty array to store the tensor format of each image in the all data dataframe\n",
    "tensor_images = []\n",
    "\n",
    "#2. Incremenet through each row and for each row using the url present call the fetch image function to get its numerical and lower res image\n",
    "for i, row in all_data.iterrows():\n",
    "    print(f\"Processing row {i}\")\n",
    "    time.sleep(0.1)\n",
    "    img_tensor = fetch_and_standardize_image(row['URL'], size=(224, 224)) \n",
    "    tensor_images.append(img_tensor)\n",
    "\n",
    "# 3. Using the populated tensor_images array now store results in the dataframe\n",
    "all_data['tensor_image'] = tensor_images\n",
    "\n",
    "#4. Remove any images that are null / had errors and print dataframe out to see if it worked\n",
    "final_data = all_data[all_data['tensor_image'].notnull()].reset_index(drop=True)\n",
    "final_data.head()\n",
    "\n",
    "#5. Save to a csv called final data, left commented out because once it is run once we do not need to resave \n",
    "final_data.to_csv('../DATA/final_joined_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to now get an arrays for the two things needed in the model (an array of the tensor photos, and then an array of the binary labels)\n",
    "\n",
    "#1. Getting an array of the tensor images (according to chat .stack is needed to enforce the shape here). Probably because nested array structure the underlying shape could be different but given checks shouldnt be an issue.\n",
    "X = np.stack(final_data['tensor_image'].values)\n",
    "\n",
    "#2. Getting an Array of the binary labels, num classes for 0 and 1\n",
    "y = to_categorical(final_data['label'].values, num_classes=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We have the two arrays needed to run the model on, now create the train test split using standard .2 test size\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Load ResNet50 model, includes the input shape that we standardized the images to be in and gets head start from imagenet trained by microsoft\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "#2. According to Chat need to preserve original weights of the model while adding ontop of it but not changing foundational aspects \n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "#3. Adding the specific painting or drawing classification\n",
    "# Chat helped and https://forums.fast.ai/t/finetuning-top-layers-of-cnn-using-keras/18013/3, details abstracted away \n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "predictions = Dense(2, activation='softmax')(x)  \n",
    "\n",
    "#4. Building the model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "#5. Model is compiled for actual use \n",
    "model.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Now that model has been created need to fit it with train and test data, epochs are the amount of rounds the model goes through the test data\n",
    "#Earlier Epochs are less likely to be accurate because weightings have not been established but after each round (epoch) it trys to update it based on performance\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=32)\n",
    "\n",
    "#2. Evaluate the accurage of the model \n",
    "accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
